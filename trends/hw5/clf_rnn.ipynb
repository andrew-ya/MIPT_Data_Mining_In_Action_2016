{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning for Natural Language Processing\n",
    "\n",
    "\n",
    " * Simple text representations, bag of words\n",
    " * Word embedding and... not just another word2vec this time\n",
    " * rnn for text\n",
    " * Aggregating several data sources \"the hard way\"\n",
    " * Solving ~somewhat~ real ML problem with ~almost~ end-to-end deep learning\n",
    " \n",
    "\n",
    "Special thanks to Irina Golzmann for help with technical part, task prepared by Александр Панин, jheuristic@yandex-team.ru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK\n",
    "\n",
    "You will require nltk v3.2 to solve this assignment\n",
    "\n",
    "__It is really important that the version is 3.2, otherwize russian tokenizer might not work__\n",
    "\n",
    "Install/update\n",
    "* `sudo pip install --upgrade nltk==3.2`\n",
    "* If you don't remember when was the last pip upgrade, `sudo pip install --upgrade pip`\n",
    "\n",
    "If for some reason you can't or won't switch to nltk v3.2, just make sure that russian words are tokenized properly with RegeExpTokenizer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For students with low-RAM machines\n",
    " * This assignment can be accomplished with even the low-tier hardware (<= 4Gb RAM) \n",
    " * If that is the case, turn flag \"low_RAM_mode\" below to True\n",
    " * If you have around 8GB memory, it is unlikely that you will feel constrained by memory.\n",
    " * In case you are using a PC from last millenia, consider setting very_low_RAM=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "low_RAM_mode = True\n",
    "very_low_RAM = False  #If you have <3GB RAM, set BOTH to true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset\n",
    "\n",
    "Ex-kaggle-competition on prohibited content detection\n",
    "\n",
    "There goes the description - https://www.kaggle.com/c/avito-prohibited-content\n",
    "\n",
    "\n",
    "### Download\n",
    "High-RAM mode,\n",
    " * Download avito_train.tsv from competition data files\n",
    "Low-RAM-mode,\n",
    " * Download downsampled dataset from here\n",
    "     * archive https://yadi.sk/d/l0p4lameqw3W8\n",
    "     * raw https://yadi.sk/d/I1v7mZ6Sqw2WK (in case you feel masochistic)\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# What's inside\n",
    "Different kinds of features:\n",
    "* 2 text fields - title and description\n",
    "* Special features - price, number of e-mails, phones, etc\n",
    "* Category and subcategory - unsurprisingly, categorical features\n",
    "* Attributes - more factors\n",
    "\n",
    "Only 1 binary target whether or not such advertisement contains prohibited materials\n",
    "* criminal, misleading, human reproduction-related, etc\n",
    "* diving into the data may result in prolonged sleep disorders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if not low_RAM_mode:\n",
    "    # a lot of ram\n",
    "    df = pd.read_csv(\"avito_train.tsv\",sep='\\t')\n",
    "else:\n",
    "    #aroung 4GB ram\n",
    "    df = pd.read_csv(\"data/avito_train_1kk.tsv\",sep='\\t')\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1204949, 13) 0.228222107326\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>itemid</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>description</th>\n",
       "      <th>attrs</th>\n",
       "      <th>price</th>\n",
       "      <th>is_proved</th>\n",
       "      <th>is_blocked</th>\n",
       "      <th>phones_cnt</th>\n",
       "      <th>emails_cnt</th>\n",
       "      <th>urls_cnt</th>\n",
       "      <th>close_hours</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10000010</td>\n",
       "      <td>Транспорт</td>\n",
       "      <td>Автомобили с пробегом</td>\n",
       "      <td>Toyota Sera, 1991</td>\n",
       "      <td>Новая оригинальная линзованая оптика на ксенон...</td>\n",
       "      <td>{\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...</td>\n",
       "      <td>150000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10000094</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Одежда, обувь, аксессуары</td>\n",
       "      <td>Костюм Steilmann</td>\n",
       "      <td>Юбка и топ из панбархата. Под топ  трикотажная...</td>\n",
       "      <td>{\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...</td>\n",
       "      <td>1500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10000299</td>\n",
       "      <td>Личные вещи</td>\n",
       "      <td>Детская одежда и обувь</td>\n",
       "      <td>Костюм Didriksons Boardman, размер 100, краги,...</td>\n",
       "      <td>Костюм Didriksons Boardman, в отличном состоян...</td>\n",
       "      <td>{\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...</td>\n",
       "      <td>3000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10000309</td>\n",
       "      <td>Недвижимость</td>\n",
       "      <td>Квартиры</td>\n",
       "      <td>1-к квартира, 44 м², 9/20 эт.</td>\n",
       "      <td>В кирпичном пан.-м доме, продается одноком.-ая...</td>\n",
       "      <td>{\"Тип объявления\":\"Продам\", \"Количество комнат...</td>\n",
       "      <td>2642020</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>22.47</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10000317</td>\n",
       "      <td>Услуги</td>\n",
       "      <td>Предложения услуг</td>\n",
       "      <td>Поездки на таможню, печать в паспорте</td>\n",
       "      <td>Поездки на таможню гражданам СНГ для пересечен...</td>\n",
       "      <td>{\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...</td>\n",
       "      <td>1500</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.43</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     itemid      category                subcategory  \\\n",
       "0  10000010     Транспорт      Автомобили с пробегом   \n",
       "1  10000094   Личные вещи  Одежда, обувь, аксессуары   \n",
       "2  10000299   Личные вещи     Детская одежда и обувь   \n",
       "3  10000309  Недвижимость                   Квартиры   \n",
       "4  10000317        Услуги          Предложения услуг   \n",
       "\n",
       "                                               title  \\\n",
       "0                                  Toyota Sera, 1991   \n",
       "1                                   Костюм Steilmann   \n",
       "2  Костюм Didriksons Boardman, размер 100, краги,...   \n",
       "3                      1-к квартира, 44 м², 9/20 эт.   \n",
       "4              Поездки на таможню, печать в паспорте   \n",
       "\n",
       "                                         description  \\\n",
       "0  Новая оригинальная линзованая оптика на ксенон...   \n",
       "1  Юбка и топ из панбархата. Под топ  трикотажная...   \n",
       "2  Костюм Didriksons Boardman, в отличном состоян...   \n",
       "3  В кирпичном пан.-м доме, продается одноком.-ая...   \n",
       "4  Поездки на таможню гражданам СНГ для пересечен...   \n",
       "\n",
       "                                               attrs    price  is_proved  \\\n",
       "0  {\"Год выпуска\":\"1991\", \"Тип кузова\":\"Купе\", \"П...   150000        NaN   \n",
       "1  {\"Вид одежды\":\"Женская одежда\", \"Предмет одежд...     1500        NaN   \n",
       "2  {\"Вид одежды\":\"Для мальчиков\", \"Предмет одежды...     3000        NaN   \n",
       "3  {\"Тип объявления\":\"Продам\", \"Количество комнат...  2642020        NaN   \n",
       "4  {\"Вид услуги\":\"Деловые услуги\", \"Тип услуги\":\"...     1500        0.0   \n",
       "\n",
       "   is_blocked  phones_cnt  emails_cnt  urls_cnt  close_hours  \n",
       "0           0           0           0         0         0.03  \n",
       "1           0           0           0         0         0.41  \n",
       "2           0           0           0         0         5.49  \n",
       "3           0           1           0         0        22.47  \n",
       "4           1           0           0         0         1.43  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print (df.shape, df.is_blocked.mean())\n",
    "df[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](https://kaggle2.blob.core.windows.net/competitions/kaggle/3929/media/Ad.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio 0.228222107326\n",
      "Count: 1204949\n"
     ]
    }
   ],
   "source": [
    "print (\"Blocked ratio\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance-out the classes\n",
    "* Vast majority of data samples are non-prohibited\n",
    " * 250k banned out of 4kk\n",
    " * Let's just downsample random 250k legal samples to make further steps less computationally demanding\n",
    " * If you aim for high Kaggle score, consider a smarter approach to that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blocked ratio: 0.504583519879\n",
      "Count: 544996\n"
     ]
    }
   ],
   "source": [
    "#downsample\n",
    "\n",
    "sample_size = 270000\n",
    "\n",
    "not_blocked = df[df['is_blocked'] == 0].sample(n=sample_size)\n",
    "\n",
    "df = pd.concat([df[df['is_blocked'] == 1], not_blocked])\n",
    "\n",
    "\n",
    "print (\"Blocked ratio:\",df.is_blocked.mean())\n",
    "print (\"Count:\",len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed\n"
     ]
    }
   ],
   "source": [
    "assert df.is_blocked.mean() < 0.51\n",
    "assert df.is_blocked.mean() > 0.49\n",
    "assert len(df) <= 560000\n",
    "\n",
    "print(\"All tests passed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In case your RAM-o-meter is in the red\n",
    "if very_low_RAM:\n",
    "    data = data[::2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Tokenizing\n",
    "\n",
    "First, we create a dictionary of all existing words.\n",
    "Assign each word a number - it's Id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.tokenize import RegexpTokenizer\n",
    "from collections import Counter,defaultdict\n",
    "tokenizer = RegexpTokenizer(r\"\\w+\")\n",
    "\n",
    "#Dictionary of tokens\n",
    "token_counts = Counter()\n",
    "\n",
    "#All texts\n",
    "all_texts = np.hstack([df.description.values,df.title.values])\n",
    "\n",
    "\n",
    "#Compute token frequencies\n",
    "for s in all_texts:\n",
    "    if type(s) is not str:\n",
    "        continue\n",
    "    s = s.lower()\n",
    "    tokens = tokenizer.tokenize(s)\n",
    "    for token in tokens:\n",
    "        token_counts[token] +=1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove rare tokens\n",
    "\n",
    "We are unlikely to make use of words that are only seen a few times throughout the corpora.\n",
    "\n",
    "Again, if you want to beat Kaggle competition metrics, consider doing something better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAFkCAYAAAAKf8APAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAAPYQAAD2EBqD+naQAAIABJREFUeJzt3X+QnmV97/H3J01DKMrG0zQJKlA7h9YIaEwoUCwFFYgt\nFnXsQFdS/HHqOcrUyUQpHLROGc5gZ0QMTIQ5B8ahYGTrqa2NsShEBSoSqZCWFggcVDT8SkjULEMg\nBJrr/HHfK0+ebvZX9sfF5v2aeSZ57uu7949rdnY/e93XdT8ppSBJklSTGVN9ApIkSd0MKJIkqToG\nFEmSVB0DiiRJqo4BRZIkVceAIkmSqmNAkSRJ1TGgSJKk6hhQJElSdQwokiSpOqMKKEk+lOSeJP3t\n644kb+uquSjJY0meSXJLktd1tc9KsirJ1iRPJ1mT5FVdNXOSfCHJ9vZ1fZKerppDk6xt97E1yRVJ\nZnbVHJ3k1vZcHknyydFcryRJmhqjHUF5BLgAWAwsAb4NfHUghCS5AFgOnAscA2wG1iU5qGMfVwDv\nAM4E3gS8DPhaknTU9AGvB04DlgKLgOsHGpPMAG4EDgROAM4C3g1c1lHzcuBm4NH2XD8CnJdkxSiv\nWZIkTbLs64cFJvkpcF4p5dokjwOfLaV8pm2bBWwBzi+lXJPkYGArcHYp5cttzSE0wef3SynrkiwE\n7gOOLaXc1dYcB6wHfquU8lCS3we+Cry6lLKlrTkLuBaYV0p5OsmHgUva9y+0NRcAf1ZKOXSfLlqS\nJE2oMc9BSTIjyR8DBwD/lOQ1wAJg3UBNKWUXcBvNKAc0oyozu2qeAO7tqDke2D4QTtqaO4H+rpp7\nB8JJ6yZgNs1oyUDNbQPhpKPmlUkOH+t1S5KkiTdz+JI9JTmKZjRjNvAMcGYp5YdJfgcoNCMmnbYA\nh7X/nw/sKqX0D1KzoP3/AuDJQQ79ZFfNHscppWxPsqur5uFBjpO27Sd7ub5fpbmt9GNg52A1kiRp\nULOBXwduKqX8dF92NOqAAjwAvAHoAf4I+JskJ+3LSVRmKfDFqT4JSZJews4GbtiXHYw6oLS3TH7U\nvv2XJMcCHwb+imZ0Yj7N5NgBne83A7OS9HSNoswH7uiomTfIoed17efYzsYkc4BZwBMdNfO79jGf\nZpRnM3v3Y4DVq1ezcOHCIco0nlasWMHKlSun+jT2K/b55LPPJ599Prk2btzIsmXLoP1dui/GMoLS\nLcAvlVIeTrIZOBW4B34xSfYk4M/b2ruBF9qazkmyRwHntTXrgZ4kx3RNkj2YF0PMeuDjSeaVUgZu\nBy2luSWzoaPmkiQzO+ahLAUeL6UMenuntRNg4cKFLF68eNSdobHp6emxvyeZfT757PPJZ59PmX2e\nIjHa56B8KsmJSQ5PclSSS2gCyOq25HKa4PDOdq7KXwM7aJYNU0p5Cvg8cFmStyR5Y/u19wDfamse\noJnMek2S45IcD1wNrC2lPNQe52bgfmB1kkVJ3gpcClxdSnm6rbkBeA64LsmRSd4FXEjHUmRJklSn\n0Y6gzAOuAw6hWVXzb8DSUsotAKWUTyeZDVwJvAK4EzitlLKjYx/LgeeBL9E8x+SbwDllz/XOvcAq\nmqACsIbmOSa0x9md5HTgKuB24FmaoHN+R81TSU5tz+X7wM+Bz5RSLh/lNUuSpEk2qoBSSvnTEdRc\nDFw8RPvzNCFl+RA1/cA5wxznUeCMYWruA04eqkaSJNXHz+JRFXp7e6f6FPY79vnks88nn33+0rXP\nT5KdbpIsBu6+++67nVglSdIobNiwgSVLlgAsKaVsGK5+KI6gSJKk6hhQJElSdQwokiSpOgYUSZJU\nHQOKJEmqjgFFkiRVx4AiSZKqY0CRJEnVMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWS\nJFXHgCJJkqpjQJEkSdUxoEiSpOoYUCRJUnUMKJIkqToGFEmSVB0DiiRJqo4BRZIkVceAIkmSqmNA\nkSRJ1TGgSJKk6hhQJElSdQwokiSpOgYUSZJUHQOKJEmqjgFFkiRVx4AiSZKqY0CRJEnVMaBIkqTq\nGFAkSVJ1Zk71CbxUbdq0iW3btg1ZM3fuXA477LBJOiNJkqYPA8oYbNq0id/6rYXs3PnMkHWzZ/8K\nDz640ZAiSdIoGVDGYNu2bW04WQ0s3EvVRnbuXMa2bdsMKJIkjZIBZZ8sBBZP9UlIkjTtOElWkiRV\nZ1QBJcmFSf45yVNJtiT5SpLf7Kq5NsnurtcdXTWzkqxKsjXJ00nWJHlVV82cJF9Isr19XZ+kp6vm\n0CRr231sTXJFkpldNUcnuTXJM0keSfLJ0VyzJEmafKMdQTkRWAUcB5xCc4vo5iQHdtV9HZgPLGhf\nf9DVfgXwDuBM4E3Ay4CvJUlHTR/weuA0YCmwCLh+oDHJDOBG4EDgBOAs4N3AZR01LwduBh4FlgAf\nAc5LsmKU1y1JkibRqOaglFL2CBpJ3g88SfPL//aOpudKKVsH20eSg4EPAGeXUm5pty0DHqEJPeuS\nLKQJJceWUu5qaz4IrE9yRCnlobb9tcAppZQtbc3HgGuTfKKU8jSwDDgAeF8p5QVgY5JPAR8FVo7m\n2iVJ0uTZ1zkoc4AC/Kxr+8ntLaAHk1yd5Nc62pbQBKN1AxtKKU8A99KMhAAcD2wfCCdtzZ1Af1fN\nvQPhpHUTMLs9xkDNbW046ax5ZZLDR321kiRpUuxrQFkJfKeUcn/HthuBs4E304xU/Dbw7SS/3LYv\nAHaVUvq79rWlbRuoeXKQ4z3ZVdMZTiilbAd2DVXTvk9HjSRJqsyYlxknuRI4kmYOyS+UUv624+39\nSe4GfgycDvzDWI832VasWEFPzx5zcunt7aW3t3eKzkiSpHr09fXR19e3x7b+/u6xh7EbU0BJsgp4\nO3Bie3tmr0opm5NsAo5oN20GZiXp6RpFmQ/c0VEzb5DdzWvbBmqO7TqvOcAs4ImOmvld+5hPc1tq\nM0NYuXIlixf7jBNJkgYz2B/tGzZsYMmSJXv5itEZ9S2eJJ8D3gm8uZSyaQT1c4FDeTE03A28AJza\nUXMIcBTw3XbTeqAnyTEdNccBB/NiiFkPHJWkM8gsBXYCGzpqfq9r6fFS4PFSyk+Gv1pJkjQVRvsc\nlKto5pe8B9iRZH77mt22H5Tk0iTHJzk8ycnAGpq5I18BKKU8BXweuCzJW5K8keaZ8fcA32prHqCZ\nzHpNkuOSHA9cDaxtV/BAs3z4fmB1kkVJ3gpcClzdruABuAF4DrguyZFJ3gVcSMdSZEmSVJ/R3uL5\nEM3tkVu7tr+f5hkl/wEcDfwJzQqfJ4BvA2eWUnZ01C8Hnge+RPMck28C55RSSkdNL80zV25q36+h\neY4JAKWU3UlOB66iWeL8LE3QOb+j5qkkpwJXAt8Hfg58ppRy+SivW5IkTaLRPgdlyBGXUspO4G0j\n2M/zNCFl+RA1/cA5w+znUeCMYWruA04e7pwkSVI9/CweSZJUHQOKJEmqjgFFkiRVx4AiSZKqY0CR\nJEnVMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEkSdUxoEiSpOoY\nUCRJUnUMKJIkqToGFEmSVB0DiiRJqo4BRZIkVceAIkmSqmNAkSRJ1TGgSJKk6hhQJElSdQwokiSp\nOgYUSZJUHQOKJEmqjgFFkiRVx4AiSZKqY0CRJEnVMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4ok\nSaqOAUWSJFXHgCJJkqpjQJEkSdUxoEiSpOoYUCRJUnUMKJIkqTqjCihJLkzyz0meSrIlyVeS/OYg\ndRcleSzJM0luSfK6rvZZSVYl2Zrk6SRrkryqq2ZOki8k2d6+rk/S01VzaJK17T62JrkiycyumqOT\n3NqeyyNJPjmaa5YkSZNvtCMoJwKrgOOAU4CZwM1JDhwoSHIBsBw4FzgG2AysS3JQx36uAN4BnAm8\nCXgZ8LUk6ajpA14PnAYsBRYB13ccZwZwI3AgcAJwFvBu4LKOmpcDNwOPAkuAjwDnJVkxyuuWJEmT\naObwJS8qpfxB5/sk7weepPnlf3u7eTlwSSllTVvzXmAL8B7gmiQHAx8Azi6l3NLWLAMeoQk965Is\npAklx5ZS7mprPgisT3JEKeWhtv21wCmllC1tzceAa5N8opTyNLAMOAB4XynlBWBjkk8BHwVWjuba\nJUnS5NnXOShzgAL8DCDJa4AFwLqBglLKLuA2mlEOaEZVZnbVPAHc21FzPLB9IJy0NXcC/V019w6E\nk9ZNwGyawDRQc1sbTjprXpnk8LFdsiRJmmj7GlBWAt8ppdzfvl9AE1i2dNVtadsA5gO7Sin9Q9Qs\noBmZ6fZkV80exymlbAd2DVXTvk9HjSRJqsyobvF0SnIlcCTNHBJJkqRxM6aAkmQV8HbgxPb2zIDN\nNKMT89v/D+h8vxmYlaSnaxRlPnBHR828QQ49r2s/x3ad1xxgFvBER838rn3Mpxnl2cwQVqxYQU/P\nHouG6O3tpbe3d6gvkyRpv9DX10dfX98e2/r7u2+OjN2oA0qSz9GswDmplLKps62U8nCSzcCpwD1t\n/SzgJODP27K7gRfami+3NYcARwHntTXrgZ4kx3RMkj0OOJgXQ8x64ONJ5pVSBm4HLQV2Ahs6ai5J\nMrNjHspS4PFSyk+Gus6VK1eyePHiEfaKJEn7l8H+aN+wYQNLlizZy1eMzmifg3IVcDbNipwdSea3\nr9kdZZfTBId3JjkK+GtgB82yYUopTwGfBy5L8pYkbwRW0wSab7U1D9BMZr0myXFJjgeuBta2K3ig\nWT58P7A6yaIkbwUuBa5uV/AA3AA8B1yX5Mgk7wIupGMpsiRJqs9oR1A+RHN75Nau7e+nfUZJKeXT\nbWC5EngFcCdwWillR0f9cuB54Es0zzH5JnBOKaV01PTSPHPlpvb9GprnmNAeZ3eS04GraJY4P0sT\ndM7vqHkqyantuXwf+DnwmVLK5aO8bkmSNIlG+xyUEY24lFIuBi4eov15mpCyfIiafuCcYY7zKHDG\nMDX3AScPVSNJkuriZ/FIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEk\nSdUxoEiSpOoYUCRJUnUMKJIkqToGFEmSVB0DiiRJqo4BRZIkVceAIkmSqmNAkSRJ1TGgSJKk6hhQ\nJElSdQwokiSpOgYUSZJUHQOKJEmqjgFFkiRVx4AiSZKqY0CRJEnVMaBIkqTqGFAkSVJ1DCiSJKk6\nBhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEkSdUxoEiSpOoYUCRJUnUMKJIkqToGFEmSVB0DiiRJ\nqo4BRZIkVceAIkmSqmNAkSRJ1TGgSJKk6ow6oCQ5MclXkzyWZHeSM7rar223d77u6KqZlWRVkq1J\nnk6yJsmrumrmJPlCku3t6/okPV01hyZZ2+5ja5Irkszsqjk6ya1JnknySJJPjvaaJUnS5BrLCMpB\nwL8C5wJlLzVfB+YDC9rXH3S1XwG8AzgTeBPwMuBrSdJR0we8HjgNWAosAq4faEwyA7gROBA4ATgL\neDdwWUfNy4GbgUeBJcBHgPOSrBjlNUuSpEk0c/iSPZVSvgF8A6ArUHR6rpSydbCGJAcDHwDOLqXc\n0m5bBjwCnAKsS7KQJpQcW0q5q635ILA+yRGllIfa9tcCp5RStrQ1HwOuTfKJUsrTwDLgAOB9pZQX\ngI1JPgV8FFg52muXJEmTY6LmoJycZEuSB5NcneTXOtqW0ASjdQMbSilPAPfSjIQAHA9sHwgnbc2d\nQH9Xzb0D4aR1EzC7PcZAzW1tOOmseWWSw/f1IiVJ0sSYiIByI3A28GaakYrfBr6d5Jfb9gXArlJK\nf9fXbWnbBmqeHGTfT3bVdIYTSinbgV1D1bTv01EjSZIqM+pbPMMppfxtx9v7k9wN/Bg4HfiH8T7e\nRFmxYgU9PXvMyaW3t5fe3t4pOiNJkurR19dHX1/fHtv6+7vHHsZu3ANKt1LK5iSbgCPaTZuBWUl6\nukZR5gN3dNTMG2R389q2gZpjOxuTzAFmAU901Mzv2sd8msm9mxnCypUrWbx48VAlkiTttwb7o33D\nhg0sWbJkL18xOhP+HJQkc4FDeTE03A28AJzaUXMIcBTw3XbTeqAnyTEdNccBB/NiiFkPHJWkM8gs\nBXYCGzpqfq9r6fFS4PFSyk/2/eokSdJEGMtzUA5K8oYki9pNv9G+P7RtuzTJ8UkOT3IysIZm7shX\nAEopTwGfBy5L8pYkbwRWA/cA32prHqCZzHpNkuOSHA9cDaxtV/BAs3z4fmB1kkVJ3gpcClzdruAB\nuAF4DrguyZFJ3gVcSMdSZEmSVJ+x3OI5BriF5jZJ4cVf9tfRPBvlaOBPgDk0oybfBs4spezo2Mdy\n4HngSzTPMfkmcE4ppfO5Kr3AKpqgAk3Q+chAYylld5LTgauA24FnaYLO+R01TyU5FbgS+D7wc+Az\npZTLx3DdkiRpkozlOSi3MfTIy9tGsI/naULK8iFq+oFzhtnPo8AZw9TcB5w83DlJkqR6+Fk8kiSp\nOgYUSZJUHQOKJEmqjgFFkiRVx4AiSZKqY0CRJEnVMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4ok\nSaqOAUWSJFXHgCJJkqpjQJEkSdUxoEiSpOoYUCRJUnUMKJIkqToGFEmSVB0DiiRJqo4BRZIkVceA\nIkmSqmNAkSRJ1TGgSJKk6hhQJElSdQwokiSpOgYUSZJUHQOKJEmqjgFFkiRVx4AiSZKqY0CRJEnV\nMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEkSdUxoEiSpOoYUCRJ\nUnUMKJIkqTqjDihJTkzy1SSPJdmd5IxBai5q259JckuS13W1z0qyKsnWJE8nWZPkVV01c5J8Icn2\n9nV9kp6umkOTrG33sTXJFUlmdtUcneTW9lweSfLJ0V6zJEmaXGMZQTkI+FfgXKB0Nya5AFjeth8D\nbAbWJTmoo+wK4B3AmcCbgJcBX0uSjpo+4PXAacBSYBFwfcdxZgA3AgcCJwBnAe8GLuuoeTlwM/Ao\nsAT4CHBekhVjuG5JkjRJZg5fsqdSyjeAbwB0BYoBy4FLSilr2pr3AluA9wDXJDkY+ABwdinllrZm\nGfAIcApNmFlIE0qOLaXc1dZ8EFif5IhSykNt+2uBU0opW9qajwHXJvlEKeVpYBlwAPC+UsoLwMYk\nnwI+Cqwc7bVLkqTJMa5zUJK8BlgArBvYVkrZBdxGM8oBzajKzK6aJ4B7O2qOB7YPhJO25k6gv6vm\n3oFw0roJmE0zWjJQc1sbTjprXpnk8LFfqSRJmkjjPUl2Ac1tny1d27e0bQDzgV2llP4hahYATw6y\n/ye7avY4TillO7BrqJr2fTpqJElSZUZ9i2d/sWLFCnp69piTS29vL729vVN0RpIk1aOvr4++vr49\ntvX3d489jN14B5TNNKMT89v/D+h8vxmYlaSnaxRlPnBHR828QfY/r2s/x3Y2JpkDzAKe6KiZ37WP\n+TSjPJsZwsqVK1m8ePFQJZIk7bcG+6N9w4YNLFmyZC9fMTrjeounlPIwzS/+Uwe2JZkFnAR8t910\nN/BCV80hwFEdNeuBniTHdNQcBxzMiyFmPXBUks4gsxTYCWzoqPm9rqXHS4HHSyk/GfuVSpKkiTSW\n56AclOQNSRa1m36jfX9o+/5y4ONJ3pnkKOCvgR00y4YppTwFfB64LMlbkrwRWA3cA3yrrXmAZjLr\nNUmOS3I8cDWwtl3BA83y4fuB1UkWJXkrcClwdbuCB+AG4DnguiRHJnkXcCEdS5ElSVJ9xnKL5xjg\nFprbJIUXf9lfB3yglPLpJLOBK4FXAHcCp5VSdnTsYznwPPAlmueYfBM4p5TS+VyVXmAVTVABWEPz\nHBMASim7k5wOXAXcDjxLE3TO76h5Ksmp7bl8H/g58JlSyuVjuG5JkjRJxvIclNsYZuSllHIxcPEQ\n7c/ThJTlQ9T0A+cMc5xHgf/0JNuumvuAk4eqkSRJdfGzeCRJUnUMKJIkqToGFEmSVB0DiiRJqo4B\nRZIkVceAIkmSqmNAkSRJ1TGgSJKk6hhQJElSdQwokiSpOgYUSZJUHQOKJEmqjgFFkiRVx4AiSZKq\nY0CRJEnVMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqozc6pPYLrbuHHj\nsDVz587lsMMOm4SzkSTppcGAMmGeAGawbNmyYStnz/4VHnxwoyFFkqSWAWXCbAd2A6uBhUPUbWTn\nzmVs27bNgCJJUsuAMuEWAoun+iQkSXpJcZKsJEmqjgFFkiRVx4AiSZKqY0CRJEnVMaBIkqTqGFAk\nSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEkSdUxoEiSpOoYUCRJUnXGPaAk+csk\nu7tej3fVXJTksSTPJLklyeu62mclWZVka5Knk6xJ8qqumjlJvpBke/u6PklPV82hSda2+9ia5Iok\nM8f7miVJ0viaqBGUe4H5wIL2dfRAQ5ILgOXAucAxwGZgXZKDOr7+CuAdwJnAm4CXAV9Lko6aPuD1\nwGnAUmARcH3HcWYANwIHAicAZwHvBi4bx+uUJEkTYKJGE14opWzdS9ty4JJSyhqAJO8FtgDvAa5J\ncjDwAeDsUsotbc0y4BHgFJows5AmlBxbSrmrrfkgsD7JEaWUh9r21wKnlFK2tDUfA65N8olSytMT\ncuWSJGmfTdQIyhHtLZwfJelL8hqA9t8FwLqBwlLKLuA2mlEOaEZVZnbVPEEzKjNQczywfSCctDV3\nAv1dNfcOhJPWTcBsYMl4XagkSRp/ExFQvgecQ3Pr5U9pAsl3k7yi/X+hGTHptKVtg+bW0K5SSv8Q\nNQuAJwc59pNdNXscp5SyHdjVUSNJkio07rd4Sik3dby9L8n3gB8C7wXuHO/jSZKk6WfCV7SUUp5J\n8u/AEcAaIDSjJJs7yjrfbwZmJenpGkWZD9zRUTNvkMPN69rPsZ2NSeYAs7qOPagVK1bQ07PHoiB6\ne3vp7e0d7kslSZr2+vr66Ovr22Nbf3/3zY+xm/CAkuQAYCFwWynl4SSbgVOBe9r2WcBJwJ+3X3I3\n8EJb8+W25hDgKOC8tmY90JPkmI5JsscBB/NiiFkPfDzJvFLKwO2gpcDO9hhDWrlyJYsXLx7zdUuS\nNJ0N9kf7hg0bWLJkfKZ5jntASXIpsBbYRDPq8RfAy3lxCfDlNMHhB8APgI8DO2iWDVNKeSrJ54HL\nkvwM+DnwGZpA86225oEkN9Gs+vkQzajM/wHWtit4AG4G7gdWJzkf+FXgUuBqV/BIklS3iRhBeTVw\nAzAX2Eozafb4UsojAKWUTyeZDVwJvIJmXspppZQdHftYDjwPfInmOSbfBM4ppZSOml5gFc3KHGhu\nH31koLGUsjvJ6cBVwO3As8Bq4PxxvVpJkjTuJmKS7LCTNEopFwMXD9H+PE1IWT5ETT/NaqGhjvMo\ncMZw5yNJkuriZ/FIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEkSdUx\noEiSpOoYUCRJUnUMKJIkqToT8WGBGoONGzcO2T537lwOO+ywSTobSZKmlgFlyj0BzGDZsmVDVs2e\n/Ss8+OBGQ4okab9gQJly24HdwGpg4V5qNrJz5zK2bdtmQJEk7RcMKNVYCCye6pOQJKkKTpKVJEnV\nMaBIkqTqGFAkSVJ1DCiSJKk6BhRJklQdA4okSaqOAUWSJFXHgCJJkqpjQJEkSdXxSbIvIX6goCRp\nf2FAeUnwAwUlSfsXA8pLgh8oKEnavxhQXlL8QEFJ0v7BSbKSJKk6BhRJklQdA4okSaqOc1CmmeGW\nIoPLkSVJ9TOgTBsjW4oMLkeWJNXPgDJtjGQpMrgcWZL0UmBAmXZGthTZp9JKkmpmQNnv+FRaSVL9\nDCj7HZ9KK0mqnwFlvzX8rSBvA0mSpooBRYMY2W2gAw6Yzd/93Zc55JBDhqwzyEiSRsuAokGM5DbQ\nd3juuY/y9re/fdi9OZ9FkjRaBhQNYajbQBsZzbLm73znOyxcuPe6tWvX8od/+IdDno0jMeOrr6+P\n3t7eqT6N/Yp9Pvns85eu/SKgJDkXOA84BLgXWFFKuX1qz2q6GG4uy8gfIHfRRRcN2T6SW0qGmJHz\nB/fks88nn33+0jXtA0qSs4CVwIeAO9p/v55kYSnl0Sk9uf3CSG4X3Qh8cpiakd1SGum8mOeee44D\nDjhgn2sMRJI0MaZ9QAFWANeUUq4deJ9kKfBh4BNTd1r7m+FuF42kZvzmxcAvAf+xzzWTHYhGUjPS\numeffXbY/UjSVJnWASXJLwNLgL/qaroZOGHyz0j7bjzmxYxkxGb8RnUa4xOIRlYzsroZM2bwj//4\nj0OGq8kOTdP9eIZCaeSmdUAB5tL8pN7StX0LsGAvXzMb4O///u+56667Bi3YtGlT+78befGv/27f\nHUHNSOvGq2a6H2+g5uEhzgfg8RHUjaTmQZpA9N9opjftzb8Da4apG6+akdY9xO7dXxpBuJpBc437\nWjOe+3rpHi+ZwRVXXMHcuXP3vpcZM9i9e+j9jKRmPPf1Uj7eY489xhe/+MWqzmk6H+/hh3/xM3P2\nsDsbRkop+7qPaiU5BHgMOKGU8r2O7RcC55RS/tOfxkneAwz93SxJkoZydinlhn3ZwXQfQdlGM849\nv2v7fGDzXr7mJuBs4MfAzgk7M0mSpp/ZwK/T/C7dJ9N6BAUgyfeAu0opf9ax7T7gH0opTpKVJKlC\n030EBeCzwPVJ7gbWA/8DOBT431N6VpIkaa+mfUAppfzfJP+FZknGwIPafr+U8sjUnpkkSdqbaX+L\nR5IkvfTMmOoTkCRJ6mZAkSRJ1TGgdEhybpIfJXk2yfeT/O5Un9N0keTEJF9N8liS3UnOGKTmorb9\nmSS3JHndVJzrdJHkwiT/nOSpJFuSfCXJbw5SZ7+PkyQfSnJPkv72dUeSt3XV2N8TKMn/bH/GfLZr\nu/0+TpKCm6NgAAADm0lEQVT8ZdvHna/Hu2r2ub8NKK2ODxX8X8Ai4HaaDxV89ZSe2PRxEPCvwLnA\nf5r4lOQCYHnbfgzNc2rWJTloMk9ymjkRWAUcB5xCMyn+5iQHDhTY7+PuEeACms9jWAJ8G/jqwA9n\n+3tiJflt4L8D93Rtt9/H3700zxRb0L6OHmgYt/4upfhqJgp/D/hc17b7gUum+tym24vmeeBndG17\nHDiv4/0s4OfAB6f6fKfLi+ajH3YDv2u/T2q//xR4v/094f38MprPn3gLcAvw2Y42+318+/ovgQ1D\ntI9LfzuCwh4fKriuq8kPFZwESV5Dk8B/0f+llF3Abdj/42kOzejVz8B+n2hJZiT5Y+AA4J/s7wl3\nJbC2lPLtzo32+4Q5or2F86MkfW0/j2t/T/vnoIzQWD5UUONnAc0vzsH6/7DJP51payXwnVLK/e17\n+30CJDmK5qGQs4FngDNLKT9M8jvY3xOiDYKLaG4ndPP7fPx9DzgH+H80t3k+CXw3yZGMY38bUKT9\nQJIrgSOBN031uewHHgDeAPQAfwT8TZKTpvaUpq92nuDlwCmllOen+nz2B6WUzs/Zua/9SJkfAu8F\n7hyv43iLpzGWDxXU+NkMBPt/QiRZBbwdOLmU8kRHk/0+AUopL5RSflRK+ZfSfN7XncCHsb8nyhLg\n14ANSZ5P8jxwErA8yS6av9zt9wlUSnkG+HfgCMbx+9yAArSp+27g1K6mU4E7Jv+M9i+llIdpvnF/\n0f9JZtH8kPnuVJ3XdJDkc8A7gTeXUjZ1ttnvkybAL9nfE+abNCtIFtGMXL0BuAtYDbyhlPIj7PcJ\nleQAYCHw+Hh+n3uL50V+qOAEapeX/VeaH9YAv5HkDcDPSvO5SJcDH0/yA+AHwMeBHUDfVJzvdJDk\nKqAXOAPYkWTgL5r+UsrO9v/2+zhK8ing68Am4OU0/X8ScFpbYn+Ps1LKDpoVl7+QZAfw01LKxnaT\n/T6OklwKrKX5Pp8P/AXN9/v1bcm49LcBpVX8UMGJdgzN0r/Svi5rt18HfKCU8ukks2lm4r+CZlj8\ntPaHj8bmQzR9fWvX9vfT/iCx38fdPJrv6UOAfuDfgKWllFvA/p5EezxryX4fd68GbqBZYLKVZtLs\n8QO/L8erv/2wQEmSVB3noEiSpOoYUCRJUnUMKJIkqToGFEmSVB0DiiRJqo4BRZIkVceAIkmSqmNA\nkSRJ1TGgSJKk6hhQJElSdQwokiSpOv8f2oWTzVJhsR4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f09c80440f0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Word frequency distribution, just for kicks\n",
    "_=plt.hist(list(token_counts.values()),range=[0,50],bins=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select only the tokens that had at least 10 occurences in the corpora.\n",
    "#Use token_counts.\n",
    "\n",
    "min_count = 10\n",
    "# tokens = <tokens from token_counts keys that had at least min_count occurences throughout the dataset>\n",
    "tokens = [token for token, cnt in token_counts.items() if cnt >= min_count]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "token_to_id = {t:i+1 for i,t in enumerate(tokens)}\n",
    "null_token = \"NULL\"\n",
    "token_to_id[null_token] = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Tokens: 87517\n"
     ]
    }
   ],
   "source": [
    "print (\"# Tokens:\",len(token_to_id))\n",
    "if len(token_to_id) < 30000:\n",
    "    print (\"Alarm! It seems like there are too few tokens. Make sure you updated NLTK and applied correct thresholds -- unless you now what you're doing, ofc\")\n",
    "if len(token_to_id) > 1000000:\n",
    "    print( \"Alarm! Too many tokens. You might have messed up when pruning rare ones -- unless you know what you're doin' ofc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replace words with IDs\n",
    "Set a maximum length for titles and descriptions.\n",
    " * If string is longer that that limit - crop it, if less - pad with zeros.\n",
    " * Thus we obtain a matrix of size [n_samples]x[max_length]\n",
    " * Element at i,j - is an identifier of word j within sample i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorize(strings, token_to_id, max_len=150):\n",
    "    token_matrix = []\n",
    "    for s in strings:\n",
    "        if type(s) is not str:\n",
    "            token_matrix.append([0]*max_len)\n",
    "            continue\n",
    "        s = s.lower()\n",
    "        tokens = tokenizer.tokenize(s)\n",
    "        token_ids = list(map(lambda token: token_to_id.get(token,0), tokens))[:max_len]\n",
    "        token_ids += [0]*(max_len - len(token_ids))\n",
    "        token_matrix.append(token_ids)\n",
    "\n",
    "    return np.array(token_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "desc_tokens = vectorize(df.description.values,token_to_id,max_len = 150)\n",
    "title_tokens = vectorize(df.title.values,token_to_id,max_len = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Data format examples\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Размер матрицы: (544996, 15)\n",
      "Поездки на таможню, печать в паспорте -> [46066 43844 30235 39532 72926 17751     0     0     0     0] ...\n",
      "Рефлекторно-урогинекологический массаж -> [55441     0 64849     0     0     0     0     0     0     0] ...\n",
      "Возьму суду под200 т. р -> [43129 11543     0  8207 29199     0     0     0     0     0] ...\n"
     ]
    }
   ],
   "source": [
    "print(\"Размер матрицы:\",title_tokens.shape)\n",
    "for title, tokens in zip(df.title.values[:3],title_tokens[:3]):\n",
    "    print (title,'->', tokens[:10],'...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ As you can see, our preprocessing is somewhat crude. Let us see if that is enough for our network __"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non-sequences\n",
    "\n",
    "\n",
    "Some data features are not text samples. E.g. price, # urls, category, etc\n",
    "\n",
    "They require a separate preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All numeric features\n",
    "df_numerical_features = df[[\"phones_cnt\",\"emails_cnt\",\"urls_cnt\",\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#One-hot-encoded category and subcategory\n",
    "\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "\n",
    "categories = []\n",
    "data_cat_subcat = df[[\"category\",\"subcategory\"]].values\n",
    "\n",
    "# categories = [A list of dictionaries {\"category\":category_name, \"subcategory\":subcategory_name} for each data sample]\n",
    "categories = [{\"category\":row[0], \"subcategory\":row[1]} for row in data_cat_subcat]\n",
    "\n",
    "\n",
    "vectorizer = DictVectorizer(sparse=False)\n",
    "cat_one_hot = vectorizer.fit_transform(categories)\n",
    "cat_one_hot = pd.DataFrame(cat_one_hot,columns=vectorizer.feature_names_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_non_text = pd.merge(\n",
    "    df_numerical_features,cat_one_hot,on = np.arange(len(cat_one_hot))\n",
    ")\n",
    "del df_non_text[\"key_0\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split data into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Target variable - whether or not sample contains prohibited material\n",
    "target = df.is_blocked.values.astype('int32')\n",
    "#Preprocessed titles\n",
    "title_tokens = title_tokens.astype('int32')\n",
    "#Preprocessed tokens\n",
    "desc_tokens = desc_tokens.astype('int32')\n",
    "\n",
    "#Non-sequences\n",
    "df_non_text = df_non_text.astype('float32').as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Split into training and test set.\n",
    "\n",
    "\n",
    "#Difficulty selector:\n",
    "#Easy: split randomly\n",
    "#Medium: select test set items that have item_ids strictly above that of training set\n",
    "#Hard: do whatever you want, but score yourself using kaggle private leaderboard\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = train_test_split(title_tokens, desc_tokens,\n",
    "                                                                                              df_non_text, target, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save preprocessed data [optional]\n",
    "\n",
    "* The next tab can be used to stash all the essential data matrices and get rid of the rest of the data.\n",
    " * Highly recommended if you have less than 1.5GB RAM left\n",
    "* To do that, you need to first run it with save_prepared_data=True, then restart the notebook and only run this tab with read_prepared_data=True."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "save_prepared_data = True #save\n",
    "read_prepared_data = False #load\n",
    "\n",
    "#but not both at once\n",
    "assert not (save_prepared_data and read_prepared_data)\n",
    "\n",
    "if save_prepared_data:\n",
    "    print (\"Saving preprocessed data (may take up to 3 minutes)\")\n",
    "\n",
    "    import pickle\n",
    "    with open(\"preprocessed_data.pcl\",'w') as fout:\n",
    "        pickle.dump(data_tuple,fout)\n",
    "    with open(\"token_to_id.pcl\",'w') as fout:\n",
    "        pickle.dump(token_to_id,fout)\n",
    "\n",
    "    print (\"готово\")\n",
    "    \n",
    "elif read_prepared_data:\n",
    "    print (\"Reading saved data...\")\n",
    "    \n",
    "    import pickle\n",
    "    \n",
    "    with open(\"preprocessed_data.pcl\",'r') as fin:\n",
    "        data_tuple = pickle.load(fin)\n",
    "    title_tr,title_ts,desc_tr,desc_ts,nontext_tr,nontext_ts,target_tr,target_ts = data_tuple\n",
    "    with open(\"token_to_id.pcl\",'r') as fin:\n",
    "        token_to_id = pickle.load(fin)\n",
    "        \n",
    "    #Re-importing libraries to allow staring noteboook from here\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    import matplotlib.pyplot as plt\n",
    "    %matplotlib inline\n",
    "   \n",
    "    print (\"done\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the monster\n",
    "\n",
    "Since we have several data sources, our neural network may differ from what you used to work with.\n",
    "\n",
    "* Separate input for titles: RNN\n",
    "* Separate input for description: RNN\n",
    "* Separate input for categorical features: обычные полносвязные слои или какие-нибудь трюки\n",
    " \n",
    "These three inputs must be blended somehow - concatenated or added.\n",
    "\n",
    "* Output: a simple binary classification\n",
    " * 1 sigmoidal with binary_crossentropy\n",
    " * 2 softmax with categorical_crossentropy - essentially the same as previous one\n",
    " * 1 neuron without nonlinearity (lambda x: x) +  hinge loss\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.gof.compilelock): Overriding existing lock by dead process '9785' (I am process '9815')\n"
     ]
    }
   ],
   "source": [
    "#libraries\n",
    "import lasagne\n",
    "from theano import tensor as T\n",
    "import theano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#3 inputs and a refere output\n",
    "title_token_ids = T.matrix(\"title_token_ids\",dtype='int32')\n",
    "desc_token_ids = T.matrix(\"desc_token_ids\",dtype='int32')\n",
    "categories = T.matrix(\"categories\",dtype='float32')\n",
    "target_y = T.ivector(\"is_blocked\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "title_inp = lasagne.layers.InputLayer((None,title_tr.shape[1]),input_var=title_token_ids)\n",
    "descr_inp = lasagne.layers.InputLayer((None,desc_tr.shape[1]),input_var=desc_token_ids)\n",
    "cat_inp = lasagne.layers.InputLayer((None,nontext_tr.shape[1]), input_var=categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Descriptions\n",
    "\n",
    "output_size = 64\n",
    "\n",
    "#word-wise embedding. We recommend to start from some 64 and improving after you are certain it works.\n",
    "descr_nn = lasagne.layers.EmbeddingLayer(descr_inp, input_size=len(token_to_id)+1, output_size=output_size)\n",
    "descr_nn = lasagne.layers.LSTMLayer(descr_nn, num_units=15)\n",
    "descr_nn = lasagne.layers.DenseLayer(descr_nn,100)\n",
    "\n",
    "# Titles\n",
    "title_nn = lasagne.layers.EmbeddingLayer(title_inp, input_size=len(token_to_id)+1, output_size=output_size)\n",
    "title_nn = lasagne.layers.LSTMLayer(title_nn, num_units=15)\n",
    "title_nn = lasagne.layers.DenseLayer(title_nn,100)\n",
    "\n",
    "# Non-sequences\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_inp,100)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_nn,100)\n",
    "cat_nn = lasagne.layers.DenseLayer(cat_nn,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 300)\n"
     ]
    }
   ],
   "source": [
    "nn = lasagne.layers.ConcatLayer([descr_nn, title_nn, cat_nn])                                  \n",
    "\n",
    "print(nn.output_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn = lasagne.layers.DenseLayer(nn, 150)\n",
    "nn = lasagne.layers.DropoutLayer(nn, p=0.5)\n",
    "nn = lasagne.layers.DenseLayer(nn,1,nonlinearity=lasagne.nonlinearities.linear)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss function\n",
    "\n",
    "* The standard way:\n",
    " * prediction\n",
    " * loss\n",
    " * updates\n",
    " * training and evaluation functions\n",
    " \n",
    " \n",
    "* Hinge loss\n",
    " * $ L_i = \\max(0, \\delta - t_i p_i) $\n",
    " * delta is a tunable parameter: how far should a neuron be in the positive margin area for us to stop bothering about it\n",
    " * Function description may mention some +-1  limitations - this is not neccessary, at least as long as hinge loss has a __default__ flag `binary = True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#All trainable params\n",
    "weights = lasagne.layers.get_all_params(nn,trainable=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Simple NN prediction\n",
    "prediction = lasagne.layers.get_output(nn)[:,0]\n",
    "\n",
    "#Hinge loss\n",
    "loss = lasagne.objectives.binary_hinge_loss(prediction,target_y,delta = 1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Weight optimization step\n",
    "updates = lasagne.updates.adadelta(loss, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determinitic prediction \n",
    " * In case we use stochastic elements, e.g. dropout or noize\n",
    " * Compile a separate set of functions with deterministic prediction (deterministic = True)\n",
    " * Unless you think there's no neet for dropout there ofc. Btw is there?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#deterministic version\n",
    "det_prediction = lasagne.layers.get_output(nn,deterministic=True)[:,0]\n",
    "\n",
    "#equivalent loss function\n",
    "det_loss = lasagne.objectives.binary_hinge_loss(det_prediction,target_y,delta = 1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coffee-lation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[loss,prediction],updates = updates)\n",
    "eval_fun = theano.function([desc_token_ids,title_token_ids,categories,target_y],[det_loss,det_prediction])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training loop\n",
    "* The regular way with loops over minibatches\n",
    "* Since the dataset is huge, we define epoch as some fixed amount of samples isntead of all dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#average precision at K\n",
    "\n",
    "from oracle import APatK, score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Out good old minibatch iterator now supports arbitrary amount of arrays (X,y,z)\n",
    "\n",
    "def iterate_minibatches(*arrays,**kwargs):\n",
    "    batchsize=kwargs.get(\"batchsize\",100)\n",
    "    shuffle = kwargs.get(\"shuffle\",True)\n",
    "    \n",
    "    if shuffle:\n",
    "        indices = np.arange(len(arrays[0]))\n",
    "        np.random.shuffle(indices)\n",
    "    for start_idx in range(0, len(arrays[0]) - batchsize + 1, batchsize):\n",
    "        if shuffle:\n",
    "            excerpt = indices[start_idx:start_idx + batchsize]\n",
    "        else:\n",
    "            excerpt = slice(start_idx, start_idx + batchsize)\n",
    "        yield [arr[excerpt] for arr in arrays]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweaking guide\n",
    "\n",
    "* batch_size - how many samples are processed per function call\n",
    "  * optimization gets slower, but more stable, as you increase it.\n",
    "  * May consider increasing it halfway through training\n",
    "* minibatches_per_epoch - max amount of minibatches per epoch\n",
    "  * Does not affect training. Lesser value means more frequent and less stable printing\n",
    "  * Setting it to less than 10 is only meaningfull if you want to make sure your NN does not break down after one epoch\n",
    "* n_epochs - total amount of epochs to train for\n",
    "  * `n_epochs = 10**10` and manual interrupting is still an option\n",
    "\n",
    "\n",
    "Tips:\n",
    "\n",
    "* With small minibatches_per_epoch, network quality may jump around 0.5 for several epochs\n",
    "\n",
    "* AUC is the most stable of all three metrics\n",
    "\n",
    "* Average Precision at top 2.5% (APatK) - is the least stable. If batch_size*minibatches_per_epoch < 10k, it behaves as a uniform random variable.\n",
    "\n",
    "* Plotting metrics over training time may be a good way to analyze which architectures work better.\n",
    "\n",
    "* Once you are sure your network aint gonna crash, it's worth letting it train for a few hours of an average laptop's time to see it's true potential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train:\n",
      "\tloss: 163.188452437\n",
      "\tacc: 0.628217821782\n",
      "\tauc: 0.664163047829\n",
      "\tap@k: 0.134324784338\n",
      "Val:\n",
      "\tloss: 160.270746341\n",
      "\tacc: 0.497623762376\n",
      "\tauc: 0.494416807089\n",
      "\tap@k: 0.539276254177\n",
      "Train:\n",
      "\tloss: 71.9873782256\n",
      "\tacc: 0.618118811881\n",
      "\tauc: 0.61617484662\n",
      "\tap@k: 0.0681505809947\n",
      "Val:\n",
      "\tloss: 103.920364773\n",
      "\tacc: 0.499306930693\n",
      "\tauc: 0.495444828161\n",
      "\tap@k: 0.463459624637\n",
      "Train:\n",
      "\tloss: 6.41352062986\n",
      "\tacc: 0.584356435644\n",
      "\tauc: 0.587562048414\n",
      "\tap@k: 0.150329714934\n",
      "Val:\n",
      "\tloss: 0.992760319393\n",
      "\tacc: 0.505841584158\n",
      "\tauc: 0.500740048078\n",
      "\tap@k: 0.668565064831\n",
      "Train:\n",
      "\tloss: 0.987904554911\n",
      "\tacc: 0.508712871287\n",
      "\tauc: 0.501280224059\n",
      "\tap@k: 0.497332401198\n",
      "Val:\n",
      "\tloss: 1.00108282529\n",
      "\tacc: 0.499306930693\n",
      "\tauc: 0.523937307046\n",
      "\tap@k: 0.637949007509\n",
      "Train:\n",
      "\tloss: 0.990954535141\n",
      "\tacc: 0.505940594059\n",
      "\tauc: 0.498807438752\n",
      "\tap@k: 0.500982563127\n",
      "Val:\n",
      "\tloss: 0.981123728848\n",
      "\tacc: 0.511881188119\n",
      "\tauc: 0.509316896905\n",
      "\tap@k: 0.73188092053\n",
      "Train:\n",
      "\tloss: 0.986239023952\n",
      "\tacc: 0.508613861386\n",
      "\tauc: 0.501823872361\n",
      "\tap@k: 0.50580836615\n",
      "Val:\n",
      "\tloss: 0.987425195775\n",
      "\tacc: 0.508613861386\n",
      "\tauc: 0.50239037713\n",
      "\tap@k: 0.564140604231\n",
      "Train:\n",
      "\tloss: 0.986063772765\n",
      "\tacc: 0.50900990099\n",
      "\tauc: 0.501148083807\n",
      "\tap@k: 0.603676981972\n",
      "Val:\n",
      "\tloss: 0.996233810681\n",
      "\tacc: 0.502178217822\n",
      "\tauc: 0.484868294334\n",
      "\tap@k: 0.447233817368\n",
      "Train:\n",
      "\tloss: 0.992333979336\n",
      "\tacc: 0.505445544554\n",
      "\tauc: 0.497748483057\n",
      "\tap@k: 0.511749122358\n",
      "Val:\n",
      "\tloss: 0.999545685357\n",
      "\tacc: 0.500297029703\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.443793355875\n",
      "Train:\n",
      "\tloss: 0.992951035541\n",
      "\tacc: 0.50495049505\n",
      "\tauc: 0.500156627451\n",
      "\tap@k: 0.520897297662\n",
      "Val:\n",
      "\tloss: 0.986495713222\n",
      "\tacc: 0.508316831683\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.491350521957\n",
      "Train:\n",
      "\tloss: 0.982743771138\n",
      "\tacc: 0.510594059406\n",
      "\tauc: 0.504714772255\n",
      "\tap@k: 0.519477980899\n",
      "Val:\n",
      "\tloss: 0.989141560636\n",
      "\tacc: 0.507623762376\n",
      "\tauc: 0.500100542932\n",
      "\tap@k: 0.576395856909\n",
      "Train:\n",
      "\tloss: 1.0053033241\n",
      "\tacc: 0.496831683168\n",
      "\tauc: 0.494691444527\n",
      "\tap@k: 0.464029723764\n",
      "Val:\n",
      "\tloss: 0.99442581813\n",
      "\tacc: 0.505247524752\n",
      "\tauc: 0.499899939964\n",
      "\tap@k: 0.469341028539\n",
      "Train:\n",
      "\tloss: 0.996279693833\n",
      "\tacc: 0.503762376238\n",
      "\tauc: 0.49382166762\n",
      "\tap@k: 0.500175280449\n",
      "Val:\n",
      "\tloss: 0.999334337211\n",
      "\tacc: 0.500495049505\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.526497950051\n",
      "Train:\n",
      "\tloss: 0.992659088755\n",
      "\tacc: 0.505742574257\n",
      "\tauc: 0.495003595416\n",
      "\tap@k: 0.443638468977\n",
      "Val:\n",
      "\tloss: 0.991479081791\n",
      "\tacc: 0.505346534653\n",
      "\tauc: 0.500002098073\n",
      "\tap@k: 0.516092054713\n",
      "Train:\n",
      "\tloss: 0.996885306699\n",
      "\tacc: 0.502574257426\n",
      "\tauc: 0.497646952626\n",
      "\tap@k: 0.529283204953\n",
      "Val:\n",
      "\tloss: 0.999136537837\n",
      "\tacc: 0.500594059406\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.45399603956\n",
      "Train:\n",
      "\tloss: 0.983700481821\n",
      "\tacc: 0.510396039604\n",
      "\tauc: 0.500973286167\n",
      "\tap@k: 0.504224951912\n",
      "Val:\n",
      "\tloss: 0.982130887789\n",
      "\tacc: 0.511584158416\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.526728716193\n",
      "Train:\n",
      "\tloss: 1.00162075197\n",
      "\tacc: 0.499900990099\n",
      "\tauc: 0.491812194562\n",
      "\tap@k: 0.508228294528\n",
      "Val:\n",
      "\tloss: 0.999157628278\n",
      "\tacc: 0.500594059406\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.540271375437\n",
      "Train:\n",
      "\tloss: 0.994177270948\n",
      "\tacc: 0.504455445545\n",
      "\tauc: 0.494787097103\n",
      "\tap@k: 0.521225591594\n",
      "Val:\n",
      "\tloss: 0.982404226415\n",
      "\tacc: 0.512277227723\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.519505240475\n",
      "Train:\n",
      "\tloss: 0.998436519724\n",
      "\tacc: 0.500297029703\n",
      "\tauc: 0.507605864855\n",
      "\tap@k: 0.52720979064\n",
      "Val:\n",
      "\tloss: 0.98591249241\n",
      "\tacc: 0.509207920792\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.495148842953\n",
      "Train:\n",
      "\tloss: 0.994257245277\n",
      "\tacc: 0.503168316832\n",
      "\tauc: 0.506969753437\n",
      "\tap@k: 0.484592856819\n",
      "Val:\n",
      "\tloss: 0.988577365701\n",
      "\tacc: 0.507128712871\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.482533275298\n",
      "Train:\n",
      "\tloss: 0.99535745901\n",
      "\tacc: 0.503663366337\n",
      "\tauc: 0.497006034752\n",
      "\tap@k: 0.504798584532\n",
      "Val:\n",
      "\tloss: 0.987854091755\n",
      "\tacc: 0.50801980198\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.475395555006\n",
      "Train:\n",
      "\tloss: 0.993091066486\n",
      "\tacc: 0.505544554455\n",
      "\tauc: 0.494626493429\n",
      "\tap@k: 0.472980251938\n",
      "Val:\n",
      "\tloss: 0.994321169064\n",
      "\tacc: 0.504653465347\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.573939652986\n",
      "Train:\n",
      "\tloss: 0.986006087504\n",
      "\tacc: 0.510594059406\n",
      "\tauc: 0.489487702959\n",
      "\tap@k: 0.45669110974\n",
      "Val:\n",
      "\tloss: 0.996233122228\n",
      "\tacc: 0.502673267327\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.470878626739\n",
      "Train:\n",
      "\tloss: 1.00275296564\n",
      "\tacc: 0.498118811881\n",
      "\tauc: 0.49793791807\n",
      "\tap@k: 0.444181922704\n",
      "Val:\n",
      "\tloss: 0.992074939312\n",
      "\tacc: 0.505841584158\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.467690811659\n",
      "Train:\n",
      "\tloss: 0.993480788762\n",
      "\tacc: 0.505544554455\n",
      "\tauc: 0.490223128702\n",
      "\tap@k: 0.499317388203\n",
      "Val:\n",
      "\tloss: 0.99383444818\n",
      "\tacc: 0.504059405941\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.476858372502\n",
      "Train:\n",
      "\tloss: 0.989419028477\n",
      "\tacc: 0.507920792079\n",
      "\tauc: 0.490414612431\n",
      "\tap@k: 0.440516460217\n",
      "Val:\n",
      "\tloss: 0.995782404155\n",
      "\tacc: 0.502574257426\n",
      "\tauc: 0.499900477707\n",
      "\tap@k: 0.512031102507\n",
      "Train:\n",
      "\tloss: 0.99451982306\n",
      "\tacc: 0.504356435644\n",
      "\tauc: 0.491817318237\n",
      "\tap@k: 0.444385195938\n",
      "Val:\n",
      "\tloss: 0.979481446754\n",
      "\tacc: 0.512772277228\n",
      "\tauc: 0.500101605365\n",
      "\tap@k: 0.5181948984\n",
      "Train:\n",
      "\tloss: 0.981095752493\n",
      "\tacc: 0.511188118812\n",
      "\tauc: 0.506097455481\n",
      "\tap@k: 0.472369674083\n",
      "Val:\n",
      "\tloss: 0.997290173935\n",
      "\tacc: 0.501683168317\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.472992399118\n",
      "Train:\n",
      "\tloss: 1.00051661185\n",
      "\tacc: 0.499702970297\n",
      "\tauc: 0.499759415659\n",
      "\tap@k: 0.512514989563\n",
      "Val:\n",
      "\tloss: 0.990418409065\n",
      "\tacc: 0.50603960396\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.430664534608\n",
      "Train:\n",
      "\tloss: 0.990928585597\n",
      "\tacc: 0.505742574257\n",
      "\tauc: 0.503119517461\n",
      "\tap@k: 0.545400678454\n",
      "Val:\n",
      "\tloss: 1.01046370014\n",
      "\tacc: 0.493465346535\n",
      "\tauc: 0.499997391967\n",
      "\tap@k: 0.449863079509\n",
      "Train:\n",
      "\tloss: 1.00160691453\n",
      "\tacc: 0.49900990099\n",
      "\tauc: 0.500164886442\n",
      "\tap@k: 0.457337050688\n",
      "Val:\n",
      "\tloss: 0.989431295947\n",
      "\tacc: 0.50702970297\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.542183479675\n",
      "Train:\n",
      "\tloss: 0.997077784241\n",
      "\tacc: 0.503069306931\n",
      "\tauc: 0.490472869108\n",
      "\tap@k: 0.504205752035\n",
      "Val:\n",
      "\tloss: 0.989903277048\n",
      "\tacc: 0.506435643564\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.555271378376\n",
      "Train:\n",
      "\tloss: 0.982646397317\n",
      "\tacc: 0.511386138614\n",
      "\tauc: 0.495231033444\n",
      "\tap@k: 0.530463767613\n",
      "Val:\n",
      "\tloss: 0.996019069329\n",
      "\tacc: 0.502574257426\n",
      "\tauc: 0.499901497242\n",
      "\tap@k: 0.468533318223\n",
      "Train:\n",
      "\tloss: 1.00149771457\n",
      "\tacc: 0.497920792079\n",
      "\tauc: 0.511486425469\n",
      "\tap@k: 0.499220817415\n",
      "Val:\n",
      "\tloss: 0.981357565507\n",
      "\tacc: 0.513267326733\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.53796532055\n",
      "Train:\n",
      "\tloss: 1.0015638054\n",
      "\tacc: 0.499801980198\n",
      "\tauc: 0.492286284448\n",
      "\tap@k: 0.492344794359\n",
      "Val:\n",
      "\tloss: 0.993454777772\n",
      "\tacc: 0.504653465347\n",
      "\tauc: 0.500099940036\n",
      "\tap@k: 0.495649348799\n",
      "Train:\n",
      "\tloss: 0.999646624726\n",
      "\tacc: 0.500594059406\n",
      "\tauc: 0.497150902752\n",
      "\tap@k: 0.48453968883\n",
      "Val:\n",
      "\tloss: 0.991102980155\n",
      "\tacc: 0.506435643564\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.580350234965\n",
      "Train:\n",
      "\tloss: 0.997245393872\n",
      "\tacc: 0.501881188119\n",
      "\tauc: 0.501747049532\n",
      "\tap@k: 0.403682536668\n",
      "Val:\n",
      "\tloss: 0.990755132664\n",
      "\tacc: 0.506138613861\n",
      "\tauc: 0.499902190923\n",
      "\tap@k: 0.447471482272\n",
      "Train:\n",
      "\tloss: 0.995682979896\n",
      "\tacc: 0.50297029703\n",
      "\tauc: 0.498602146532\n",
      "\tap@k: 0.509602422268\n",
      "Val:\n",
      "\tloss: 0.993915393469\n",
      "\tacc: 0.504059405941\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.483974746951\n",
      "Train:\n",
      "\tloss: 0.99692537302\n",
      "\tacc: 0.50198019802\n",
      "\tauc: 0.50086986954\n",
      "\tap@k: 0.480934661374\n",
      "Val:\n",
      "\tloss: 0.988007206093\n",
      "\tacc: 0.507920792079\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.492461931785\n",
      "Train:\n",
      "\tloss: 0.988775644683\n",
      "\tacc: 0.506435643564\n",
      "\tauc: 0.506918409187\n",
      "\tap@k: 0.51109235451\n",
      "Val:\n",
      "\tloss: 1.00318065916\n",
      "\tacc: 0.497920792079\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.498984326105\n",
      "Train:\n",
      "\tloss: 0.980260911202\n",
      "\tacc: 0.51297029703\n",
      "\tauc: 0.493938063763\n",
      "\tap@k: 0.561211263536\n",
      "Val:\n",
      "\tloss: 0.976679778556\n",
      "\tacc: 0.514356435644\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.483143117268\n",
      "Train:\n",
      "\tloss: 0.998796707208\n",
      "\tacc: 0.501485148515\n",
      "\tauc: 0.494022748951\n",
      "\tap@k: 0.557732374044\n",
      "Val:\n",
      "\tloss: 0.985585394986\n",
      "\tacc: 0.509603960396\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.520051541292\n",
      "Train:\n",
      "\tloss: 1.00151450077\n",
      "\tacc: 0.499306930693\n",
      "\tauc: 0.496905728787\n",
      "\tap@k: 0.412932373347\n",
      "Val:\n",
      "\tloss: 0.993243121634\n",
      "\tacc: 0.504653465347\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.43292612677\n",
      "Train:\n",
      "\tloss: 0.983768087169\n",
      "\tacc: 0.51\n",
      "\tauc: 0.502551397973\n",
      "\tap@k: 0.481854502798\n",
      "Val:\n",
      "\tloss: 0.998881382038\n",
      "\tacc: 0.500693069307\n",
      "\tauc: 0.500099147333\n",
      "\tap@k: 0.519442779972\n",
      "Train:\n",
      "\tloss: 0.997374632447\n",
      "\tacc: 0.502574257426\n",
      "\tauc: 0.491578190642\n",
      "\tap@k: 0.471603398202\n",
      "Val:\n",
      "\tloss: 0.979708994143\n",
      "\tacc: 0.512673267327\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.571680991976\n",
      "Train:\n",
      "\tloss: 0.994816798967\n",
      "\tacc: 0.502772277228\n",
      "\tauc: 0.505585977038\n",
      "\tap@k: 0.477438937594\n",
      "Val:\n",
      "\tloss: 0.984685300993\n",
      "\tacc: 0.509603960396\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.490125705873\n",
      "Train:\n",
      "\tloss: 0.998070085334\n",
      "\tacc: 0.502178217822\n",
      "\tauc: 0.490953519126\n",
      "\tap@k: 0.426410539846\n",
      "Val:\n",
      "\tloss: 0.979174591259\n",
      "\tacc: 0.513069306931\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.461697012628\n",
      "Train:\n",
      "\tloss: 0.977140503199\n",
      "\tacc: 0.513861386139\n",
      "\tauc: 0.499137029145\n",
      "\tap@k: 0.472676164034\n",
      "Val:\n",
      "\tloss: 0.98453713176\n",
      "\tacc: 0.509306930693\n",
      "\tauc: 0.5\n",
      "\tap@k: 0.59368059089\n",
      "Train:\n",
      "\tloss: 0.989995440463\n",
      "\tacc: 0.505742574257\n",
      "\tauc: 0.503600592585\n",
      "\tap@k: 0.463020555492\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-1d0f8aa9bd24>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m         iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mminibatches_per_epoch\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mpred_probas\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb_desc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_title\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_cat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mb_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mb_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_subset\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_subset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.4/dist-packages/theano/scan_module/scan_op.py\u001b[0m in \u001b[0;36mrval\u001b[0;34m(p, i, o, n, allow_gc)\u001b[0m\n\u001b[1;32m    949\u001b[0m         \u001b[0mallow_gc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallow_gc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 951\u001b[0;31m         def rval(p=p, i=node_input_storage, o=node_output_storage, n=node,\n\u001b[0m\u001b[1;32m    952\u001b[0m                  allow_gc=allow_gc):\n\u001b[1;32m    953\u001b[0m             \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "\n",
    "n_epochs = 100\n",
    "batch_size = 100\n",
    "minibatches_per_epoch = 100\n",
    "\n",
    "for i in range(n_epochs):\n",
    "    #training\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    \n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_tr,title_tr,nontext_tr,target_tr,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch:break\n",
    "            \n",
    "        loss,pred_probas = train_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "    \n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Train:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))\n",
    "    \n",
    "    #evaluation\n",
    "    epoch_y_true = []\n",
    "    epoch_y_pred = []\n",
    "    b_c = b_loss = 0\n",
    "    for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "        iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "        if j > minibatches_per_epoch: break\n",
    "        loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "        \n",
    "        b_loss += loss\n",
    "        b_c +=1\n",
    "        \n",
    "        epoch_y_true.append(b_y)\n",
    "        epoch_y_pred.append(pred_probas)\n",
    "\n",
    "    epoch_y_true = np.concatenate(epoch_y_true)\n",
    "    epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "    \n",
    "    print (\"Val:\")\n",
    "    print ('\\tloss:',b_loss/b_c)\n",
    "    print ('\\tacc:',accuracy_score(epoch_y_true,epoch_y_pred>0.))\n",
    "    print ('\\tauc:',roc_auc_score(epoch_y_true,epoch_y_pred))\n",
    "    print ('\\tap@k:',APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \n"
     ]
    }
   ],
   "source": [
    "print(\"If you are seeing this, it's time to backup your notebook. No, really, 'tis too easy to mess up everything without noticing. \")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final evaluation\n",
    "Evaluate network over the entire test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#evaluation\n",
    "epoch_y_true = []\n",
    "epoch_y_pred = []\n",
    "\n",
    "b_c = b_loss = 0\n",
    "for j, (b_desc,b_title,b_cat, b_y) in enumerate(\n",
    "    iterate_minibatches(desc_ts,title_ts,nontext_tr,target_ts,batchsize=batch_size,shuffle=True)):\n",
    "    loss,pred_probas = eval_fun(b_desc,b_title,b_cat,b_y)\n",
    "\n",
    "    b_loss += loss\n",
    "    b_c +=1\n",
    "\n",
    "    epoch_y_true.append(b_y)\n",
    "    epoch_y_pred.append(pred_probas)\n",
    "\n",
    "\n",
    "epoch_y_true = np.concatenate(epoch_y_true)\n",
    "epoch_y_pred = np.concatenate(epoch_y_pred)\n",
    "\n",
    "final_accuracy = accuracy_score(epoch_y_true,epoch_y_pred>0)\n",
    "final_auc = roc_auc_score(epoch_y_true,epoch_y_pred)\n",
    "final_apatk = APatK(epoch_y_true,epoch_y_pred,K = int(len(epoch_y_pred)*0.025)+1)\n",
    "\n",
    "print \"Scores:\"\n",
    "print '\\tloss:',b_loss/b_c\n",
    "print '\\tacc:',final_accuracy\n",
    "print '\\tauc:',final_auc\n",
    "print '\\tap@k:',final_apatk\n",
    "score(final_accuracy,final_auc,final_apatk)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main task\n",
    "\n",
    "* https://goo.gl/forms/eJwIeAbjxzVuo6vn1\n",
    "* Feel like Le'Cun:\n",
    " * accuracy > 0.95\n",
    " * AUC > 0.97\n",
    " * Average Precision at (test sample size * 0.025) > 0.99\n",
    " * And perhaps even farther\n",
    "\n",
    "* Casual mode\n",
    " * accuracy > 0.90\n",
    " * AUC > 0.95\n",
    " * Average Precision at (test sample size * 0.025) > 0.92\n",
    "\n",
    "* Remember the training, Luke\n",
    " * Dropout, regularization\n",
    " * Mommentum, RMSprop, ada*\n",
    " * etc etc etc\n",
    " \n",
    " * If you have background in texts, there may be a way to improve tokenizer, add some lemmatization, etc etc.\n",
    " * In case you know how not to shoot yourself in the foot with RNNs, they too may be of some use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
